{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 19 Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet_df Acquisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import requests\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = twint.Config()\n",
    "# c.Search = 'Nuclear Energy'\n",
    "# c.Limit = 5000\n",
    "# c.Store_json = True\n",
    "# c.Output = 'twit_data.json'\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = twint.Config()\n",
    "# c.Search = 'Nuclear Power'\n",
    "# c.Limit = 5000\n",
    "# c.Store_json = True\n",
    "# c.Output = 'twit_data.json'\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataremoved Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'twit_data.json', 'rb')\n",
    "data = [json.loads(line) for line in f]\n",
    "tweet_df = pd.DataFrame(data)\n",
    "tweet_df = tweet_df[['tweet','link']]\n",
    "tweet_df = tweet_df.drop_duplicates()\n",
    "dataremoved = tweet_df[~tweet_df.iloc[:,0].str.contains('Ukraine')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukraine')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Ukrainian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukrainian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Ukrainians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukrainians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Zaporizhzhia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('war')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russians')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% RANDOMLY PULLING TRAINING TWEETS\n",
    "\n",
    "traindf = dataremoved.sample(frac=0.1, random_state=0)\n",
    "\n",
    "#%% REMAINDER DATA FRAME\n",
    "\n",
    "remainderdf = dataremoved.loc[~dataremoved.index.isin(traindf.index)]\n",
    "\n",
    "# INITIAL PORTIONS\n",
    "trainingset_1 = traindf[:222]\n",
    "trainingset_2 = traindf[222:444]\n",
    "trainingset_3 = traindf[444:666]\n",
    "trainingset_4 = traindf[666:]\n",
    "\n",
    "# EXPORTED TO CSV\n",
    "\n",
    "\n",
    "# THEN WE PULLED MORE\n",
    "trainingset_5_6 = remainderdf.sample(444, random_state=0)\n",
    "#updating remainder set\n",
    "remainderdf = remainderdf.loc[~remainderdf.index.isin(trainingset_5_6.index)]\n",
    "\n",
    "# pulled another\n",
    "trainingset_7_8 = remainderdf.sample(444, random_state=0)\n",
    "# updating remainder set\n",
    "remainderdf = remainderdf.loc[~remainderdf.index.isin(trainingset_7_8.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we exported the sets and labelled the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingset_1.to_csv('trainingset_1.csv',encoding='utf-8-sig', header=None)\n",
    "# trainingset_2.to_csv('trainingset_2.csv',encoding='utf-8-sig', header=None)\n",
    "# trainingset_3.to_csv('trainingset_3.csv',encoding='utf-8-sig', header=None)\n",
    "# trainingset_4.to_csv('trainingset_4.csv',encoding='utf-8-sig', header=None)\n",
    "# trainingset_5_6.to_csv('trainingset_5_6.csv',encoding='utf-8-sig', header=None)\n",
    "# trainingset_7_8.to_csv('trainingset_7_8.csv',encoding='utf-8-sig', header=None)\n",
    "\n",
    "# remainderdf.to_csv('remainder.csv',encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual labelling and dropping non-English tweets in excel, we end up with three sets of files: The manually labelled set to be used as a training set for k-NN, the unlabelled remainder set to be used as a test set for k-NN, and the combined set of all tweets to be used in LDA to distinguish topic distributions that apply to the full data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sets have been manually looked at and labelled, they are imported into the code as .csv files for further cleaning necessary for the algorithms. (all.csv, labelled.csv, and the remainder.csv) Starting with LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('labelled.csv',header=None)\n",
    "data2 = pd.read_csv('remainder.csv',header=None)\n",
    "\n",
    "#%%\n",
    "\n",
    "data = pd.concat([data1,data2])\n",
    "#%%\n",
    "data = data.iloc[:,1]\n",
    "data.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\L\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets = []\n",
    "words = []\n",
    "for tweet in data:\n",
    "    clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", tweet)\n",
    "    clean = re.sub(r\"\\d\", '', clean)\n",
    "    clean = re.sub(r\"'\\S+\", '', clean)\n",
    "    clean = clean.replace('.', '').replace(';', '').lower()\n",
    "    words += re.findall(r\"(?:\\w+|'|â€™)+\", clean)\n",
    "    cleaned_tweets.append(clean)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "standardized = [w for w in words if w not in stopwords]\n",
    "\n",
    "# removing other symbols\n",
    "corpus = [[re.sub('[^a-zA-Z ]', ' ', document)] for document in cleaned_tweets]\n",
    "#tokenizing\n",
    "corpus_tokenized = [nltk.word_tokenize(document[0]) for document in corpus]\n",
    "# stop words\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "corpus_tokenized = [[word for word in document if word not in stopwords] for document in corpus_tokenized]\n",
    "# lemmatizing\n",
    "nltk.download('wordnet')\n",
    "corpus_lemmatized = [[nltk.WordNetLemmatizer().lemmatize(word) for word in document] for document in corpus_tokenized]\n",
    "# stitching back together\n",
    "corpus = [' '.join(document) for document in corpus_lemmatized]\n",
    "# string obj for VADER\n",
    "corpus_string = \"\"\n",
    "for tweet in corpus:\n",
    "    corpus_string += tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN INPUT FOR PER WORD\n",
    "labelled = pd.DataFrame(corpus[:1175])\n",
    "labelled = labelled.join(data1[2])\n",
    "remainder = pd.Series(corpus[1175:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Preliminary LDA\n",
    "\n",
    "We will be using the gensim package's LDA model because it seems to have more LDA-specific features such as coherence score calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "#import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(corpus_lemmatized)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in corpus_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m LDA \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mldamodel\u001b[39m.\u001b[39mLdaModel\n\u001b[0;32m      3\u001b[0m \u001b[39m# Build LDA model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m lda_model \u001b[39m=\u001b[39m LDA(corpus\u001b[39m=\u001b[39;49mdoc_term_matrix, id2word\u001b[39m=\u001b[39;49mdictionary, num_topics\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                 chunksize\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, passes\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,iterations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:520\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m use_numpy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    519\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 520\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(corpus, chunks_as_numpy\u001b[39m=\u001b[39;49muse_numpy)\n\u001b[0;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    523\u001b[0m     msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrained \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:1005\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1002\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPROGRESS: pass \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, at document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1003\u001b[0m         pass_, chunk_no \u001b[39m*\u001b[39m chunksize \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(chunk), lencorpus\n\u001b[0;32m   1004\u001b[0m     )\n\u001b[1;32m-> 1005\u001b[0m     gammat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_estep(chunk, other)\n\u001b[0;32m   1007\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_alpha:\n\u001b[0;32m   1008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_alpha(gammat, rho())\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:767\u001b[0m, in \u001b[0;36mLdaModel.do_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n\u001b[1;32m--> 767\u001b[0m gamma, sstats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(chunk, collect_sstats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    768\u001b[0m state\u001b[39m.\u001b[39msstats \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sstats\n\u001b[0;32m    769\u001b[0m state\u001b[39m.\u001b[39mnumdocs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gamma\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]  \u001b[39m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:718\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    714\u001b[0m lastgamma \u001b[39m=\u001b[39m gammad\n\u001b[0;32m    715\u001b[0m \u001b[39m# We represent phi implicitly to save memory and time.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# Substituting the value of the optimal phi back into\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[39m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m gammad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m+\u001b[39m expElogthetad \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mdot(cts \u001b[39m/\u001b[39;49m phinorm, expElogbetad\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    719\u001b[0m Elogthetad \u001b[39m=\u001b[39m dirichlet_expectation(gammad)\n\u001b[0;32m    720\u001b[0m expElogthetad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(Elogthetad)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=2, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic display\n",
    "posterior = lda_model.print_topics()\n",
    "two_topic_LDA = pd.DataFrame(posterior)[1]\n",
    "two_topic_LDA = two_topic_LDA.transpose()\n",
    "two_topic_LDA.index = ['topic ' + str(i) for i in range(0,2)]\n",
    "two_topic_LDA.name = 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity and coherence scores\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix,total_docs=10000))  # a measure of how good the model is. lower the better.\n",
    "# Compute Coherence Score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_lemmatized, dictionary=dictionary , coherence='u_mass')\n",
    "if __name__ == \"__main__\":\n",
    "    #freeze_support()\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify what number of LDA topics would work best as an input to k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GRAPH FUNCTION\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LDA(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build list of scores across different topic numbers\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=corpus_lemmatized, start=2, limit=50, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=50; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number = 2??? hahahaha\n",
    "\n",
    "INTERACTIVE ELEMENT CAN GO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "pyLDAvis.show(vis_data, open_browser=False, local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MATRIX TIME, THIS OUTPUT WILL GO INTO K-NN\n",
    "\n",
    "tweet_vectors = pd.Series(0)\n",
    "for i in range(len(doc_term_matrix)):    \n",
    "    tweet_vectors[i] = lda_model.get_document_topics(doc_term_matrix[i], minimum_probability=0, minimum_phi_value=None, per_word_topics=False)\n",
    "\n",
    "tweet_vectors_entries = [[tweet_vectors[i][0][1],tweet_vectors[i][1][1]] for i in range(len(tweet_vectors))]\n",
    "\n",
    "LDA_tweet_frame = pd.DataFrame(tweet_vectors_entries, columns = ['Topic 0','Topic 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MATRIX TIME, THIS OUTPUT WILL GO INTO K-NN\n",
    "\n",
    "tweet_vectors = pd.Series(0)\n",
    "for i in range(len(doc_term_matrix)):    \n",
    "    tweet_vectors[i] = lda_model.get_document_topics(doc_term_matrix[i], minimum_probability=0, minimum_phi_value=None, per_word_topics=False)\n",
    "\n",
    "#%%\n",
    "\n",
    "tweet_vectors_entries = [[tweet_vectors[i][0][1],tweet_vectors[i][1][1]] for i in range(len(tweet_vectors))]\n",
    "\n",
    "LDA_tweet_frame = pd.DataFrame(tweet_vectors_entries, columns = ['Topic 0','Topic 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDA_tweet_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Separating the sets again (training and labelled) to go into k-NN\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m labelled_lda \u001b[39m=\u001b[39m LDA_tweet_frame\u001b[39m.\u001b[39miloc[:\u001b[39m1175\u001b[39m,:]\n\u001b[0;32m      4\u001b[0m remainder_lda \u001b[39m=\u001b[39m LDA_tweet_frame\u001b[39m.\u001b[39miloc[\u001b[39m1175\u001b[39m:,:]\n\u001b[0;32m      6\u001b[0m \u001b[39m#%%\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LDA_tweet_frame' is not defined"
     ]
    }
   ],
   "source": [
    "# Separating the sets again (training and labelled) to go into k-NN\n",
    "\n",
    "labelled_lda = LDA_tweet_frame.iloc[:1175,:]\n",
    "remainder_lda = LDA_tweet_frame.iloc[1175:,:]\n",
    "\n",
    "#%%\n",
    "labelled_lda = labelled.join(data1[2])\n",
    "#%%\n",
    "labelled_lda.columns = ['Topic 0','Topic 1','label']\n",
    "\n",
    "#%%\n",
    "\n",
    "# labelled.to_csv('labelled_LDA_vectors_withStopwords.csv',header=None)\n",
    "# remainder.to_csv('remainder_LDA_vectors_withStopwords.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH PER WORD TOKENIZATION, WITH N-GRAMS, WITH LDA VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['tweets', 'label']\n",
    "data = pd.read_csv(\"labelled.csv\", names=colnames, header=None)\n",
    "\n",
    "# FOR LDA VECTORS:\n",
    "# data = pd.read_csv(\"labelled_LDA_vectors_withStopwords.csv\", names=colnames, header=None)\n",
    "# data = pd.read_csv(\"remainder_LDA_vectors_withStopwords.csv\", names=colnames, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_tweets(data):\n",
    "    # 1. create a CountVectorizer\n",
    "    vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "    # convert the type of \"tweets\" to str\n",
    "    data[\"tweets\"] = data[\"tweets\"].astype(str)\n",
    "    tweet_list = list(data['tweets'])\n",
    "    freq = vec.fit_transform(tweet_list)\n",
    "    # create one-hot encoding\n",
    "    ohot = Binarizer().fit_transform(freq)\n",
    "    # one-hot encoding\n",
    "    corpus_binary = ohot.todense()\n",
    "\n",
    "    # convert matrix to dataframe\n",
    "    encoder_df = pd.DataFrame(corpus_binary)\n",
    "\n",
    "    # create x and y for knn\n",
    "    x = encoder_df\n",
    "    y = data['label']\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "# create x, y for knn\n",
    "x = one_hot_encoding_tweets(data)[0]\n",
    "y = one_hot_encoding_tweets(data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the optimal k for the accuracy\n",
    "k_range = range(1, 31)\n",
    "k_error = []\n",
    "k_acc = []\n",
    "optimal_k = 0\n",
    "min_error = 1\n",
    "max_acc = 0\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # the value of cv decides the ratio of training and testing data\n",
    "    scores = cross_val_score(knn, x, y, cv=4, scoring='accuracy')\n",
    "    # error rate\n",
    "    error_rate = 1 - scores.mean()\n",
    "    # record the best performance with value of k\n",
    "    if error_rate < min_error:\n",
    "        min_error = error_rate\n",
    "        optimal_k = k\n",
    "        max_acc = scores.mean()\n",
    "    k_error.append(error_rate)\n",
    "    # accuracy rate\n",
    "    k_acc.append(scores.mean())\n",
    "\n",
    "# plot: x is the k value, y is the error value\n",
    "plt.plot(k_range, k_error)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Error')\n",
    "plt.show()\n",
    "print(\"the optimal k is: \", optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the effect of random_state for accuracy\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Split dataset into training set and test set\n",
    "# test which test_size produces the largest acc\n",
    "test_size_range = range(0.1,0.4,0.1)\n",
    "acc = 0\n",
    "test_size_acc = []\n",
    "test_size = []\n",
    "for size in test_size_range:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=size, random_state=0) # 80% training and 20% test\n",
    "    # Create KNN Classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=25)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    # calculate the accuracy\n",
    "    acc_s = metrics.accuracy_score(y_test, y_pred)\n",
    "    if acc <= acc_s:\n",
    "        acc = acc_s\n",
    "        print(\"test_size: \", size)\n",
    "        print(\"Accuracy:\", acc)\n",
    "        test_size_acc.append(acc)\n",
    "        test_size.append(size)\n",
    "\n",
    "plt.plot(test_size, test_size_acc)\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('acc for KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict lables for test data using training data\n",
    "def pred_label_for_test(train_data, data_test):\n",
    "    merge_data = train_data.append(data_test, ignore_index=True)\n",
    "\n",
    "    # %% for\n",
    "    x = one_hot_encoding_tweets(merge_data)[0]\n",
    "    y = one_hot_encoding_tweets(merge_data)[1]\n",
    "    \n",
    "    # convert 'negative', 'neutral', 'positive' into 3,2,1     \n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y)\n",
    "\n",
    "    # %% for labelled.csv and remainder.csv\n",
    "    X_train = x.loc[:1174, :]\n",
    "    y_train = y[:1175]\n",
    "\n",
    "    X_test = x.loc[1175:, :]\n",
    "\n",
    "    # Create KNN Classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    return y_pred, y_train\n",
    "\n",
    "\n",
    "# drawing bar plots for data\n",
    "def bar_plot_for_data(data_for_bar):\n",
    "    req = np.array(np.unique(data_for_bar, return_counts=True)).T\n",
    "    freq_data = sorted(\n",
    "                        [(name, float(val)) for name, val in freq],\n",
    "                         key=lambda x:x[1],\n",
    "                         reverse=True\n",
    "                        )\n",
    "\n",
    "    colors_list = ['Red', 'Orange', 'Blue']\n",
    "    p1 = plt.bar(*zip(*freq_data), color=colors_list)\n",
    "\n",
    "    n = len(data_for_bar)\n",
    "    for rect1 in p1:\n",
    "        height = rect1.get_height()\n",
    "        plt.annotate(\"{}%\".format(round(height/n, 2)), (rect1.get_x() + rect1.get_width()/2,\n",
    "                                            height+.05), ha=\"center\", va=\"bottom\", fontsize=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['tweets', 'label']\n",
    "data_labelled = pd.read_csv('labelled.csv', names=colnames, header=None)\n",
    "data_labelled_cleaned = pd.read_csv('labelled_cleaned.csv', names=colnames, header=None)\n",
    "colnames = ['tweets']\n",
    "data_test = pd.read_csv(\"remainder.csv\", names=colnames, header=None)\n",
    "data_test['label'] = \"\"   # add one empty column \"label\"\n",
    "\n",
    "# obtain the predication labels\n",
    "y_pred_labelled = pred_label_for_test(data_labelled, data_test)[0]\n",
    "y_pred_labelled_cleand = pred_label_for_test(data_labelled_cleaned, data_test)[0]\n",
    "\n",
    "# draw barplots\n",
    "bar_plot_for_data(y_pred_labelled)\n",
    "bar_plot_for_data(y_pred_labelled_cleand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plots with percentages\n",
    "def pred_label_for_lda(train_lda, test_lda):\n",
    "    # %% for labelled_LDA_vectors.csv and remainder_LDA_vectors.csv\n",
    "    X_train = train_lda.loc[:1174, :1]\n",
    "    y_train = train_lda.loc[:1174, 2]\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y_train)\n",
    "    y_train = label_encoder.transform(y_train)+1\n",
    "\n",
    "    X_test = test_lda.loc[1175:, :]\n",
    "\n",
    "    # Create KNN Classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_stopw = pd.read_csv(\"labelled_LDA_vectors_withStopwords.csv\", header=None, skiprows=1)\n",
    "test_data_stopw = pd.read_csv(\"remainder_LDA_vectors_withStopwords.csv\", header=None, skiprows=1)\n",
    "\n",
    "train_data = pd.read_csv(\"labelled_LDA_vectors.csv\", header=None, skiprows=1)\n",
    "test_data = pd.read_csv(\"remainder_LDA_vectors.csv\", header=None, skiprows=1)\n",
    "\n",
    "# obtain the predication labels\n",
    "y_pred_lda = pred_label_for_lda(train_data_stopw, test_data_stopw)[0]\n",
    "y_pred_lda_cleand = pred_label_for_lda(train_data, test_data)[0]\n",
    "\n",
    "# draw barplots\n",
    "bar_plot_for_data(y_pred_lda)\n",
    "bar_plot_for_data(y_pred_lda_cleand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(corpus_string, \"english\")\n",
    "score = SentimentIntensityAnalyzer().polarity_scores(corpus_string)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2afbdf73fbcdecb6073a7d3fe1b85cb5ab8042f504d040d34130e4f01d9f1260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
